{"name":"Magellan","tagline":"Geo Spatial Data Analytics on Spark","body":"# Magellan: Geospatial Analytics Using Spark\r\n\r\nGeospatial data is pervasive, and spatial context is a very rich signal of user intent and relevance\r\nin search and targeted advertising and an important variable in many predictive analytics applications.\r\nFor example when a user searches for “canyon hotels”, without location awareness the top result\r\nor sponsored ads might be for hotels in the town “Canyon, TX”.\r\nHowever, if they are are near the Grand Canyon, the top results or ads should be for nearby hotels.\r\nThus a search term combined with location context allows for much more relevant results and ads.\r\nSimilarly a variety of other predictive analytics problems can leverage location as a context.\r\n\r\nTo leverage spatial context in a predictive analytics application requires us to be able\r\nto parse these datasets at scale, join them with target datasets that contain point in space information,\r\nand answer geometrical queries efficiently.\r\n\r\nMagellan is an open source library Geospatial Analytics using Spark as the underlying engine.\r\nWe leverage Catalyst’s pluggable optimizer to efficiently execute spatial joins, SparkSQL’s powerful operators to express geometric queries in a natural DSL, and Pyspark’s Python integration to provide Python bindings.\r\n\r\n# Linking\r\n\r\nYou can link against this library using the following coordinates:\r\n\r\n\tgroupId: org.apache\r\n\tartifactId: magellan\r\n\tversion: 1.0.0\r\n\r\n# Requirements\r\n\r\nThis library requires Spark 1.3+\r\n\r\n# Capabilities\r\n\r\nThe library currently supports the [ESRI](https://www.esri.com/library/whitepapers/pdfs/shapefile.pdf) format files.\r\n\r\nWe aim to support the full suite of [OpenGIS Simple Features for SQL ](http://www.opengeospatial.org/standards/sfs) spatial predicate functions and operators together with additional topological functions.\r\n\r\ncapabilities include:\r\n\r\n**Geometries**: Point, LineString, Polygon, MultiPoint, MultiLineString, MultiPolygon, GeometryCollection\r\n\t\r\n**Predicates**: Intersects, Touches, Disjoint, Crosses, Within, Contains, Overlaps, Equals, Covers\r\n\t\r\n**Operations**: Union, Distance, Intersection, Symmetric Difference, Convex Hull, Envelope, Buffer, Simplify, Valid, Area, Length\r\n\t\r\n**Scala and Python API**\r\n\r\n\r\n# Examples\r\n\r\n## Reading Data\r\n\r\nYou can read data as follows:\r\n\r\n\r\n\tval df = sqlCtx.load(\"org.apache.magellan\", path)\r\n\tdf.show()\r\n\t\r\n\t+-----+--------+--------------------+--------------------+-----+\r\n\t|point|polyline|             polygon|            metadata|valid|\r\n\t+-----+--------+--------------------+--------------------+-----+\r\n\t| null|    null|Polygon(5, Vector...|Map(neighborho ->...| true|\r\n\t| null|    null|Polygon(5, Vector...|Map(neighborho ->...| true|\r\n\t| null|    null|Polygon(5, Vector...|Map(neighborho ->...| true|\r\n\t| null|    null|Polygon(5, Vector...|Map(neighborho ->...| true|\r\n\t+-----+--------+--------------------+--------------------+-----+\r\n\t\r\n\tdf.select(df.metadata['neighborho']).show()\r\n\t\r\n\t+--------------------+\r\n\t|metadata[neighborho]|\r\n\t+--------------------+\r\n\t|Twin Peaks       ...|\r\n\t|Pacific Heights  ...|\r\n\t|Visitacion Valley...|\r\n\t|Potrero Hill     ...|\r\n\t+--------------------+\r\n\t\r\n\r\n\r\n# Operations\r\n\r\n## Expressions\r\n\r\n### point\r\n\r\nThis is a convenience function for creating a point from two expressions, or two literals as the case may be.\r\n\r\n\tfrom magellan.types import Point\r\n\tfrom pyspark.sql import Row, SQLContext\r\n\r\n\tRecord = Row(\"id\", \"point\")\r\n\tdf = sc.parallelize([(0, Point(1.0, 1.0)),\r\n                         (1, Point(1.0, 2.0)),\r\n                         (2, Point(-1.5, 1.5)),\r\n                         (3, Point(3.5, 0.0))]) \\\r\n    .map(lambda x: Record(*x)).toDF()\r\n\r\n\tdf.show()\r\n\t\r\n\t+---+----------+\r\n\t| id|     point|\r\n\t+---+----------+\r\n\t|  0| [1.0,1.0]|\r\n\t|  1| [1.0,2.0]|\r\n\t|  2|[-1.5,1.5]|\r\n\t|  3| [3.5,0.0]|\r\n\t+---+----------+\r\n\r\nSimilar UDFs exist for line, polygon, polyline etc.\r\n\r\n\r\n## Predicates\r\n\r\n### within\r\n\r\n\t\r\n\tPointRecord = Row(\"id\", \"point\")\r\n\tpointdf = sc.parallelize([(0, Point(1.0, 1.0)),\r\n                     \t\t   (1, Point(1.0, 2.0)),\r\n                     \t\t   (2, Point(-1.5, 1.5)),\r\n                     \t\t   (3, Point(3.5, 0.0))]) \\\r\n    \t\t\t.map(lambda x: PointRecord(*x)).toDF()\r\n\t\r\n\tPolygonRecord = Row(\"id\", \"polygon\")\r\n\tpolygondf = sc.parallelize([\r\n\t\t\t\t\t(0, Polygon([0], \t[Point(2.5, 2.5), Point(2.5, 2.75), Point(2.75, 2.75), Point(2.5, 2.5)])),\r\n                    (1, Polygon([0], [Point(0.5, 0.5), Point(0.5, 0.75), Point(0.75, 0.75), Point(0.5, 0.5)])),\r\n                    (2, Polygon([0], [Point(0.0, 0.0), Point(0.0, 1.0), Point(1.0, 1.0), Point(1.0, 0.0), Point(0.0, 0.0)]))]) \\\r\n   \t\t\t\t\t .map(lambda x: PolygonRecord(*x)).toDF()\r\n    \r\n\tfrom magellan.column import within\r\n\tfrom pyspark.sql.functions import col\r\n\t\t\t\t\t\t\t\t\t\t\t\t  \r\n    pointdf.join(polygondf).where(col(\"point\").within(col(\"polygon\"))).show()\r\n\t\t\t\t\t\t\t\t\t\t\t\r\n\t+--+-----+--+-------+\r\n\t|id|point|id|polygon|\r\n\t+--+-----+--+-------+\r\n\t+--+-----+--+-------+\r\n\r\nThis agrees with our expectation: The point 1,1 is on the boundary of one of the polygons, but that is not the same as being within the polygon.\r\n\r\n\r\n### intersects\r\n\r\n    pointdf.join(polygondf).where(col(\"point\").intersects(col(\"polygon\"))).show()\t\r\n\r\n\t+--+-----------+--+--------------------+\r\n\t|id|      point|id|             polygon|\r\n\t+--+-----------+--+--------------------+\r\n\t| 0|[1,1.0,1.0]| 2|[5,ArrayBuffer(0)...|\r\n\t+--+-----------+--+--------------------+","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}